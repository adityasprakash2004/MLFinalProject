{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d1354-e24d-4847-9806-de0660959afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print(\"Train shape:\", train.shape, \" Test shape:\", test.shape)\n",
    "print(\"Train columns:\", train.columns.tolist())\n",
    "print(\"Test columns:\", test.columns.tolist())\n",
    "\n",
    "train = train.drop(['Name', 'Outcome Time', 'Outcome Subtype'] if 'Outcome Subtype' in train.columns else ['Name','Outcome Time'], axis=1)\n",
    "train = train.drop('Date of Birth', axis=1)\n",
    "test = test.drop('Date of Birth', axis=1)\n",
    "print(\"Train columns after drop:\", train.columns.tolist())\n",
    "print(\"Test columns after drop:\", test.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a302eb-573e-45d0-b71d-5dcdc8c58460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing values\n",
    "train['Sex upon Intake'] = train['Sex upon Intake'].fillna('Unknown')\n",
    "missing_age_count = train['Age upon Intake'].isna().sum()\n",
    "if missing_age_count > 0:\n",
    "    # Drop the one record with missing age\n",
    "    train = train[~train['Age upon Intake'].isna()]\n",
    "print(\"Remaining missing values in train:\", train.isnull().sum().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3efe2-83ac-44c0-9ca5-a66c76d452db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Age Parsing\n",
    "def age_to_days(age_str):\n",
    "    if pd.isna(age_str):\n",
    "        return None\n",
    "    age_str = age_str.lower().strip()\n",
    "    if age_str == 'unknown':\n",
    "        return None\n",
    "    parts = age_str.split()\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    num, unit = parts\n",
    "    try:\n",
    "        num = int(num)\n",
    "    except:\n",
    "        num = 0\n",
    "    unit = unit.rstrip('s')\n",
    "    if unit == 'day':\n",
    "        return num\n",
    "    elif unit == 'week':\n",
    "        return num * 7\n",
    "    elif unit == 'month':\n",
    "        return num * 30\n",
    "    elif unit == 'year':\n",
    "        return num * 365\n",
    "    return None\n",
    "\n",
    "# Convert Age upon Intake to AgeDays\n",
    "train['AgeDays'] = train['Age upon Intake'].apply(age_to_days)\n",
    "test['AgeDays'] = test['Age upon Intake'].apply(age_to_days)\n",
    "print(train[['Age upon Intake','AgeDays']].head(3))\n",
    "# Drop the original age\n",
    "train = train.drop('Age upon Intake', axis=1)\n",
    "test = test.drop('Age upon Intake', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5eb2b-fd82-45a2-b678-5dd1794d403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['IntakeDatetime'] = pd.to_datetime(train['Intake Time'])\n",
    "test['IntakeDatetime'] = pd.to_datetime(test['Intake Time'])\n",
    "\n",
    "train['IntakeYear'] = train['IntakeDatetime'].dt.year\n",
    "train['IntakeMonth'] = train['IntakeDatetime'].dt.month\n",
    "train['IntakeHour'] = train['IntakeDatetime'].dt.hour\n",
    "train['IntakeDow'] = train['IntakeDatetime'].dt.dayofweek  # 0=Monday\n",
    "train['IsWeekend'] = train['IntakeDow'].isin([5,6]).astype(int)\n",
    "\n",
    "test['IntakeYear'] = test['IntakeDatetime'].dt.year\n",
    "test['IntakeMonth'] = test['IntakeDatetime'].dt.month\n",
    "test['IntakeHour'] = test['IntakeDatetime'].dt.hour\n",
    "test['IntakeDow'] = test['IntakeDatetime'].dt.dayofweek\n",
    "test['IsWeekend'] = test['IntakeDow'].isin([5,6]).astype(int)\n",
    "\n",
    "# Drop original datetime\n",
    "train = train.drop(['Intake Time','IntakeDatetime'], axis=1)\n",
    "test = test.drop(['Intake Time','IntakeDatetime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a24ffa-b331-416a-9cf4-22cb00d14af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_breed(breed):\n",
    "    breed = breed.strip()\n",
    "    #special case black/tan hound\n",
    "    breed = breed.replace(\"Black/Tan Hound\", \"Black Tan Hound\")\n",
    "    breed = re.sub(r'\\s+Mix$', '', breed, flags=re.IGNORECASE)\n",
    "    breed = breed.strip()\n",
    "    if \"/\" in breed:\n",
    "        parts = breed.split(\"/\")\n",
    "    else:\n",
    "        parts = [breed]\n",
    "    if len(parts) > 2:\n",
    "        parts = [\"/\".join(parts[:-1]), parts[-1]]\n",
    "    primary = parts[0].strip()\n",
    "    secondary = parts[1].strip() if len(parts) > 1 else None\n",
    "    is_mix = 1 if (\"mix\" in breed.lower() or len(parts) > 1) else 0\n",
    "    # If mixed set secondary as Unknown\n",
    "    if is_mix and secondary is None:\n",
    "        secondary = \"Unknown\"\n",
    "    if secondary is None:\n",
    "        secondary = \"None\"\n",
    "    return primary, secondary, is_mix\n",
    "\n",
    "# Apply breed parsing to train and test\n",
    "breed_info_train = train['Breed'].apply(process_breed)\n",
    "train['Breed_Primary'] = breed_info_train.apply(lambda x: x[0])\n",
    "train['Breed_Secondary'] = breed_info_train.apply(lambda x: x[1])\n",
    "train['IsMix'] = breed_info_train.apply(lambda x: x[2])\n",
    "\n",
    "breed_info_test = test['Breed'].apply(process_breed)\n",
    "test['Breed_Primary'] = breed_info_test.apply(lambda x: x[0])\n",
    "test['Breed_Secondary'] = breed_info_test.apply(lambda x: x[1])\n",
    "test['IsMix'] = breed_info_test.apply(lambda x: x[2])\n",
    "\n",
    "# Head output\n",
    "print(train[['Breed','Breed_Primary','Breed_Secondary','IsMix']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96da6a5-fbf5-4478-b44b-fd736a57bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rare primary breeds as Other\n",
    "primary_counts = train['Breed_Primary'].value_counts()\n",
    "common_primaries = set(primary_counts[primary_counts >= 100].index)\n",
    "train['Breed_Primary_Cat'] = train['Breed_Primary'].apply(lambda x: x if x in common_primaries else \"Other\")\n",
    "test['Breed_Primary_Cat'] = test['Breed_Primary'].apply(lambda x: x if x in common_primaries else \"Other\")\n",
    "\n",
    "# Group rare secondary breeds as Other\n",
    "secondary_counts = train['Breed_Secondary'].value_counts()\n",
    "common_secondaries = set(secondary_counts[secondary_counts >= 100].index)\n",
    "train['Breed_Secondary_Cat'] = train['Breed_Secondary'].apply(lambda x: x if x in common_secondaries else \"Other\")\n",
    "test['Breed_Secondary_Cat'] = test['Breed_Secondary'].apply(lambda x: x if x in common_secondaries else \"Other\")\n",
    "\n",
    "print(\"Unique primary breed categories (grouped):\", train['Breed_Primary_Cat'].nunique())\n",
    "print(\"Unique secondary breed categories (grouped):\", train['Breed_Secondary_Cat'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9df0f-1d1e-4a93-8c3f-10d17e83204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\"tabby\",\"brindle\",\"tortie\",\"torbie\",\"calico\",\"tricolor\",\"merle\",\"point\",\"tick\"]\n",
    "\n",
    "def process_color(color):\n",
    "    color = color.strip().lower()\n",
    "    pattern_flags = {p: (1 if p in color else 0) for p in patterns}\n",
    "    parts = [part.strip() for part in color.split('/')]\n",
    "    if len(parts) > 2:\n",
    "        parts = [parts[0], parts[1]]\n",
    "    primary_part = parts[0]\n",
    "    secondary_part = parts[1] if len(parts) > 1 else None\n",
    "    def remove_pattern_words(col):\n",
    "        if col is None: \n",
    "            return None\n",
    "        for pat in [\"tabby\",\"brindle\",\"merle\",\"point\",\"tick\"]:\n",
    "            if col.endswith(\" \" + pat):\n",
    "                col = col[: -len(pat) - 1]\n",
    "        return col.strip()\n",
    "    base_primary = remove_pattern_words(primary_part).title() if primary_part else \"Unknown\"\n",
    "    base_secondary = remove_pattern_words(secondary_part).title() if secondary_part else None\n",
    "    if base_secondary is None:\n",
    "        base_secondary = \"None\"\n",
    "    is_multi = 1 if len(parts) > 1 else 0\n",
    "    if len(parts) == 1 and any(p in color for p in [\"calico\",\"tricolor\",\"tortie\",\"torbie\"]):\n",
    "        is_multi = 1\n",
    "    return base_primary, base_secondary, is_multi, pattern_flags\n",
    "\n",
    "# Apply color parsing\n",
    "color_info_train = train['Color'].apply(process_color)\n",
    "train['Color_Primary'] = color_info_train.apply(lambda x: x[0])\n",
    "train['Color_Secondary'] = color_info_train.apply(lambda x: x[1])\n",
    "train['IsMultiColor'] = color_info_train.apply(lambda x: x[2])\n",
    "for p in patterns:\n",
    "    train['Pattern_'+p.capitalize()] = color_info_train.apply(lambda x: x[3][p])\n",
    "\n",
    "color_info_test = test['Color'].apply(process_color)\n",
    "test['Color_Primary'] = color_info_test.apply(lambda x: x[0])\n",
    "test['Color_Secondary'] = color_info_test.apply(lambda x: x[1])\n",
    "test['IsMultiColor'] = color_info_test.apply(lambda x: x[2])\n",
    "for p in patterns:\n",
    "    test['Pattern_'+p.capitalize()] = color_info_test.apply(lambda x: x[3][p])\n",
    "\n",
    "# Show sample of parsed color features\n",
    "print(train[['Color','Color_Primary','Color_Secondary','IsMultiColor','Pattern_Tabby','Pattern_Calico']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da00c7-edc1-4d81-8af5-a728feb9279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_city(location):\n",
    "    loc = location.strip()\n",
    "    if \"Outside\" in loc:\n",
    "        return \"Outside\"\n",
    "    if \" in \" in loc:\n",
    "        city = loc[loc.rfind(\" in \") + 4 : loc.rfind(\" (\")]\n",
    "        return city\n",
    "    if loc.endswith(\"(TX)\"):\n",
    "        return loc[:loc.rfind(\" (\")]\n",
    "    return loc\n",
    "\n",
    "train['Found_City'] = train['Found Location'].apply(extract_city)\n",
    "test['Found_City'] = test['Found Location'].apply(extract_city)\n",
    "\n",
    "# Min threshold for city freq\n",
    "min_threshold = 100\n",
    "city_counts = train['Found_City'].value_counts()\n",
    "cities_to_keep = city_counts[city_counts >= min_threshold].index.tolist()\n",
    "\n",
    "def group_city(city, cities_to_keep=cities_to_keep):\n",
    "    return city if city in cities_to_keep else \"Other Found City\"\n",
    "\n",
    "# Create a new grouped column for the found city\n",
    "train['Found_City_Grouped'] = train['Found_City'].apply(group_city)\n",
    "test['Found_City_Grouped'] = test['Found_City'].apply(group_city)\n",
    "\n",
    "print(train['Found_City_Grouped'].value_counts())\n",
    "\n",
    "train['Found_In_Austin'] = (train['Found_City'] == 'Austin').astype(int)\n",
    "test['Found_In_Austin'] = (test['Found_City'] == 'Austin').astype(int)\n",
    "\n",
    "train = train.drop(['Found_City'], axis=1)\n",
    "test = test.drop(['Found_City'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d19c1d-57a2-4686-8b14-dbd3b7c890d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle sex and neuter status\n",
    "def split_sex(sex_str):\n",
    "    s = sex_str.lower()\n",
    "    if s.startswith(\"neutered\") or s.startswith(\"spayed\"):\n",
    "        fixed = \"Yes\"\n",
    "    elif s.startswith(\"intact\"):\n",
    "        fixed = \"No\"\n",
    "    else:\n",
    "        fixed = \"Unknown\"\n",
    "    if \"female\" in s:\n",
    "        gender = \"Female\"\n",
    "    elif \"male\" in s:\n",
    "        gender = \"Male\"\n",
    "    else:\n",
    "        gender = \"Unknown\"\n",
    "    return gender, fixed\n",
    "\n",
    "train[['Gender','Fixed']] = pd.DataFrame(train['Sex upon Intake'].apply(split_sex).tolist(), index=train.index)\n",
    "test[['Gender','Fixed']] = pd.DataFrame(test['Sex upon Intake'].apply(split_sex).tolist(), index=test.index)\n",
    "train = train.drop('Sex upon Intake', axis=1)\n",
    "test = test.drop('Sex upon Intake', axis=1)\n",
    "print(train[['Gender','Fixed']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb41304-8ef0-491b-8b48-29a6d1a09782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target var: Outcome Type\n",
    "y = train['Outcome Type']\n",
    "\n",
    "# Drop all unncecesary columns\n",
    "columns_to_drop = [\n",
    "    'Outcome Type',\n",
    "    'Breed',          \n",
    "    'Color',          \n",
    "    'Found Location', \n",
    "    'Id',            \n",
    "    'Breed_Primary', \n",
    "    'Breed_Secondary',\n",
    "    'Color_Primary',\n",
    "    'Color_Secondary'\n",
    "]\n",
    "\n",
    "train_features = train.drop(columns=columns_to_drop, axis=1)\n",
    "test_features = test.drop(columns=[col for col in columns_to_drop if col != 'Outcome Type'], axis=1)\n",
    "\n",
    "\n",
    "print(\"Remaining object columns:\", train_features.select_dtypes(include=['object']).columns.tolist())\n",
    "\n",
    "#One hot encode\n",
    "full_data = pd.concat([train_features, test_features], axis=0, ignore_index=True)\n",
    "categorical_cols = ['Animal Type','Intake Type','Intake Condition','Found_City_Grouped',\n",
    "                    'Breed_Primary_Cat','Breed_Secondary_Cat','Gender','Fixed',\n",
    "                    'IntakeYear','IntakeMonth']\n",
    "full_dummies = pd.get_dummies(full_data, columns=categorical_cols, drop_first=False)\n",
    "print(\"Total features after one-hot:\", full_dummies.shape[1])\n",
    "\n",
    "\n",
    "X_train_enc = full_dummies.iloc[:len(train_features), :].copy()\n",
    "X_test_enc = full_dummies.iloc[len(train_features):, :].copy()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_enc)\n",
    "X_test_scaled = scaler.transform(X_test_enc)\n",
    "\n",
    "#Removed below due to worse balanced accuracy\n",
    "# Apply PCA (retain 95% variance)\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=0.95, random_state=42)\n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_test_pca = pca.transform(X_test_scaled)\n",
    "# print(\"Original feature count:\", X_train_enc.shape[1], \n",
    "#       \"Reduced feature count:\", X_train_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258b584-f600-4e86-8fd7-55cc86a5f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "# Encode target labels to numeric indices\n",
    "classes = sorted(y.unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "y_num = y.map(class_to_idx)\n",
    "\n",
    "# Compute class weights\n",
    "class_counts = y_num.value_counts().to_dict()\n",
    "num_classes = len(classes)\n",
    "total_samples = len(y_num)\n",
    "class_weights = {cls_idx: total_samples/(num_classes*count) for cls_idx, count in class_counts.items()}\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y_num, test_size=0.2, stratify=y_num, random_state=42)\n",
    "\n",
    "# train LightGBM model\n",
    "model = LGBMClassifier(n_estimators=200, class_weight=class_weights, random_state=42)\n",
    "model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='multi_logloss')\n",
    "\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_preds = model.predict(X_val)\n",
    "print(\"Balanced Accuracy on val:\", balanced_accuracy_score(y_val, val_preds))\n",
    "print(\"Validation classification report:\")\n",
    "print(classification_report(y_val, val_preds, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c18cd-535e-4b3e-a9dd-fbfd0a5200ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all training data(X_train_pca switched to X_train_scaled)\n",
    "final_model = LGBMClassifier(n_estimators=300, class_weight=class_weights, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd43533-e613-4935-b2fe-b8182f212050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_pred_idx = final_model.predict(X_test_scaled)\n",
    "\n",
    "idx_to_class = {idx: cls for cls, idx in class_to_idx.items()}\n",
    "test_pred_labels = [idx_to_class[i] for i in test_pred_idx]\n",
    "\n",
    "\n",
    "output = pd.DataFrame({'Id': test['Id'], 'Outcome Type': test_pred_labels})\n",
    "print(output.head(5))\n",
    "\n",
    "output.to_csv('next_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d4ea8b-6c16-40ba-9b86-3645d95af9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
