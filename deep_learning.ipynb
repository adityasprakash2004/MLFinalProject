{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd10e90-7d55-4b1f-a4de-85659c5cc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c65efd-988f-4da4-b513-318ad02ddc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train = train.drop(['Name', 'Outcome Time', 'Outcome Subtype'] if 'Outcome Subtype' in train.columns else ['Name','Outcome Time'], axis=1)\n",
    "\n",
    "#Should we drop Date of Birth or convert it?\n",
    "train = train.drop('Date of Birth', axis=1)\n",
    "test = test.drop('Date of Birth', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14ebe8-301b-4235-b226-32f19e803eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing values\n",
    "train['Sex upon Intake'] = train['Sex upon Intake'].fillna('Unknown')\n",
    "missing_age_count = train['Age upon Intake'].isna().sum()\n",
    "if missing_age_count > 0:\n",
    "    # Drop the one record with missing age\n",
    "    train = train[~train['Age upon Intake'].isna()]\n",
    "print(\"Remaining missing values in train:\", train.isnull().sum().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967d2d1-6107-4008-956c-fa841237eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def age_to_days(age_str):\n",
    "    if pd.isna(age_str):\n",
    "        return None\n",
    "    age_str = age_str.lower().strip()\n",
    "    if age_str == 'unknown':\n",
    "        return None \n",
    "    # Split into number and unit\n",
    "    parts = age_str.split()\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    num, unit = parts\n",
    "    try:\n",
    "        num = int(num)\n",
    "    except:\n",
    "        num = 0\n",
    "    unit = unit.rstrip('s')\n",
    "    # Convert to days\n",
    "    if unit == 'day':\n",
    "        return num\n",
    "    elif unit == 'week':\n",
    "        return num * 7\n",
    "    elif unit == 'month':\n",
    "        return num * 30\n",
    "    elif unit == 'year':\n",
    "        return num * 365\n",
    "    return None\n",
    "\n",
    "# Apply conversion to Age upon Intake\n",
    "train['AgeDays'] = train['Age upon Intake'].apply(age_to_days)\n",
    "test['AgeDays'] = test['Age upon Intake'].apply(age_to_days)\n",
    "print(train[['Age upon Intake','AgeDays']].head(3))\n",
    "# Drop the original age text column\n",
    "train = train.drop('Age upon Intake', axis=1)\n",
    "test = test.drop('Age upon Intake', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f73ca5-9ea6-4019-bffb-2ba6a52a1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Intake Time to datetime and extract components\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert 'Intake Time' to datetime and extract components\n",
    "train['IntakeDatetime'] = pd.to_datetime(train['Intake Time'])\n",
    "test['IntakeDatetime'] = pd.to_datetime(test['Intake Time'])\n",
    "\n",
    "# Extract basic time components\n",
    "train['IntakeYear'] = train['IntakeDatetime'].dt.year\n",
    "train['IntakeMonth'] = train['IntakeDatetime'].dt.month\n",
    "train['IntakeHour'] = train['IntakeDatetime'].dt.hour\n",
    "train['IntakeDow'] = train['IntakeDatetime'].dt.dayofweek  # 0=Monday\n",
    "train['IsWeekend'] = train['IntakeDow'].isin([5, 6]).astype(int)\n",
    "\n",
    "test['IntakeYear'] = test['IntakeDatetime'].dt.year\n",
    "test['IntakeMonth'] = test['IntakeDatetime'].dt.month\n",
    "test['IntakeHour'] = test['IntakeDatetime'].dt.hour\n",
    "test['IntakeDow'] = test['IntakeDatetime'].dt.dayofweek\n",
    "test['IsWeekend'] = test['IntakeDow'].isin([5, 6]).astype(int)\n",
    "\n",
    "\n",
    "# Add a 'Season' feature based on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "train['Season'] = train['IntakeMonth'].apply(get_season)\n",
    "test['Season'] = test['IntakeMonth'].apply(get_season)\n",
    "\n",
    "#Cover periodic nature of time!\n",
    "train['Month_sin'] = np.sin(2 * np.pi * train['IntakeMonth'] / 12)\n",
    "train['Month_cos'] = np.cos(2 * np.pi * train['IntakeMonth'] / 12)\n",
    "train['Hour_sin'] = np.sin(2 * np.pi * train['IntakeHour'] / 24)\n",
    "train['Hour_cos'] = np.cos(2 * np.pi * train['IntakeHour'] / 24)\n",
    "\n",
    "test['Month_sin'] = np.sin(2 * np.pi * test['IntakeMonth'] / 12)\n",
    "test['Month_cos'] = np.cos(2 * np.pi * test['IntakeMonth'] / 12)\n",
    "test['Hour_sin'] = np.sin(2 * np.pi * test['IntakeHour'] / 24)\n",
    "test['Hour_cos'] = np.cos(2 * np.pi * test['IntakeHour'] / 24)\n",
    "\n",
    "# Drop original datetime; they have served their purpose with honor\n",
    "train = train.drop(['Intake Time', 'IntakeDatetime'], axis=1)\n",
    "test = test.drop(['Intake Time', 'IntakeDatetime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e95f51-ade7-477d-a2ac-4fbe27eb14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breed\n",
    "def process_breed(breed):\n",
    "    breed = breed.strip()\n",
    "    # special case \"Black/Tan Hound\" is one breed!\n",
    "    breed = breed.replace(\"Black/Tan Hound\", \"Black Tan Hound\")\n",
    "    breed = re.sub(r'\\s+Mix$', '', breed, flags=re.IGNORECASE)\n",
    "    breed = breed.strip()\n",
    "    if \"/\" in breed:\n",
    "        parts = breed.split(\"/\")\n",
    "    else:\n",
    "        parts = [breed]\n",
    "    # If more than 2 parts combine last parts\n",
    "    if len(parts) > 2:\n",
    "        parts = [\"/\".join(parts[:-1]), parts[-1]]\n",
    "    primary = parts[0].strip()\n",
    "    secondary = parts[1].strip() if len(parts) > 1 else None\n",
    "    # Determine if mixed\n",
    "    is_mix = 1 if (\"mix\" in breed.lower() or len(parts) > 1) else 0\n",
    "    # If labeled mix with no second breed, set secondary as Unknown\n",
    "    if is_mix and secondary is None:\n",
    "        secondary = \"Unknown\"\n",
    "    if secondary is None:\n",
    "        secondary = \"None\"\n",
    "    return primary, secondary, is_mix\n",
    "\n",
    "# Assuming 'train' and 'test' are your DataFrames\n",
    "breed_info_train = train['Breed'].apply(process_breed)\n",
    "train['Breed_Primary'] = breed_info_train.apply(lambda x: x[0])\n",
    "train['Breed_Secondary'] = breed_info_train.apply(lambda x: x[1])\n",
    "train['IsMix'] = breed_info_train.apply(lambda x: x[2])\n",
    "\n",
    "breed_info_test = test['Breed'].apply(process_breed)\n",
    "test['Breed_Primary'] = breed_info_test.apply(lambda x: x[0])\n",
    "test['Breed_Secondary'] = breed_info_test.apply(lambda x: x[1])\n",
    "test['IsMix'] = breed_info_test.apply(lambda x: x[2])\n",
    "\n",
    "\n",
    "# Group rare primary breeds\n",
    "primary_counts = train['Breed_Primary'].value_counts()\n",
    "common_primaries = set(primary_counts[primary_counts >= 100].index)\n",
    "train['Breed_Primary_Cat'] = train['Breed_Primary'].apply(lambda x: x if x in common_primaries else \"Other\")\n",
    "test['Breed_Primary_Cat'] = test['Breed_Primary'].apply(lambda x: x if x in common_primaries else \"Other\")\n",
    "\n",
    "# Similarly for secondary breeds\n",
    "secondary_counts = train['Breed_Secondary'].value_counts()\n",
    "common_secondaries = set(secondary_counts[secondary_counts >= 100].index)\n",
    "train['Breed_Secondary_Cat'] = train['Breed_Secondary'].apply(lambda x: x if x in common_secondaries else \"Other\")\n",
    "test['Breed_Secondary_Cat'] = test['Breed_Secondary'].apply(lambda x: x if x in common_secondaries else \"Other\")\n",
    "\n",
    "print(\"Unique primary breed categories (grouped):\", train['Breed_Primary_Cat'].nunique())\n",
    "print(\"Unique secondary breed categories (grouped):\", train['Breed_Secondary_Cat'].nunique())\n",
    "\n",
    "# Primary breed embedding index\n",
    "primary_categories = sorted(train['Breed_Primary_Cat'].unique())\n",
    "primary_mapping = {cat: idx for idx, cat in enumerate(primary_categories)}\n",
    "train['Breed_Primary_Emb'] = train['Breed_Primary_Cat'].map(primary_mapping)\n",
    "test['Breed_Primary_Emb'] = test['Breed_Primary_Cat'].map(primary_mapping)\n",
    "\n",
    "# Secondary breed embedding index\n",
    "secondary_categories = sorted(train['Breed_Secondary_Cat'].unique())\n",
    "secondary_mapping = {cat: idx for idx, cat in enumerate(secondary_categories)}\n",
    "train['Breed_Secondary_Emb'] = train['Breed_Secondary_Cat'].map(secondary_mapping)\n",
    "test['Breed_Secondary_Emb'] = test['Breed_Secondary_Cat'].map(secondary_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79065c7-3d36-41d5-997d-b3172c02861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Color\n",
    "patterns = [\"tabby\", \"brindle\", \"tortie\", \"torbie\", \"calico\", \"tricolor\", \"merle\", \"point\", \"tick\"]\n",
    "\n",
    "# Standardize colors; Cream and white are treated as the same, for eg(Later realized that dimensionality is low enough that this is not really necessary)\n",
    "def standardize_color(c):\n",
    "    # Define a mapping for common color variations.\n",
    "    mapping = {\n",
    "        \"Black\": \"Black\",\n",
    "        \"Grey\": \"Gray\",\n",
    "        \"Gray\": \"Gray\",\n",
    "        \"Brown\": \"Brown\",\n",
    "        \"Chocolate\": \"Brown\",\n",
    "        \"Liver\": \"Brown\",\n",
    "        \"Tan\": \"Tan\",\n",
    "        \"Fawn\": \"Tan\",\n",
    "        \"White\": \"White\",\n",
    "        \"Cream\": \"White\",\n",
    "        \"Ivory\": \"White\",\n",
    "        \"Orange\": \"Orange\",\n",
    "        \"Red\": \"Red\",\n",
    "        \"Golden\": \"Golden\",\n",
    "        \"Gold\": \"Golden\"\n",
    "    }\n",
    "    c_std = c.title().strip()\n",
    "    return mapping.get(c_std, c_std)\n",
    "\n",
    "# Process the Color field\n",
    "def process_color(color):\n",
    "    color = color.strip().lower()\n",
    "    pattern_flags = {p: (1 if p in color else 0) for p in patterns}\n",
    "    parts = [part.strip() for part in color.split('/')]\n",
    "    if len(parts) > 2:\n",
    "        parts = [parts[0], parts[1]]\n",
    "    primary_part = parts[0]\n",
    "    secondary_part = parts[1] if len(parts) > 1 else None\n",
    "    \n",
    "    # Remove common pattern words\n",
    "    def remove_pattern_words(col):\n",
    "        if col is None:\n",
    "            return None\n",
    "        for pat in [\"tabby\", \"brindle\", \"merle\", \"point\", \"tick\"]:\n",
    "            if col.endswith(\" \" + pat):\n",
    "                col = col[:-len(pat)-1]\n",
    "        return col.strip()\n",
    "    \n",
    "    base_primary = remove_pattern_words(primary_part)\n",
    "    if base_primary:\n",
    "        base_primary = base_primary.title()\n",
    "        base_primary = standardize_color(base_primary)\n",
    "    else:\n",
    "        base_primary = \"Unknown\"\n",
    "    \n",
    "    base_secondary = remove_pattern_words(secondary_part)\n",
    "    if base_secondary:\n",
    "        base_secondary = base_secondary.title()\n",
    "        base_secondary = standardize_color(base_secondary)\n",
    "    else:\n",
    "        base_secondary = \"None\"\n",
    "    \n",
    "    # Multicolor indicator\n",
    "    is_multi = 1 if len(parts) > 1 else 0\n",
    "    if len(parts) == 1 and any(p in color for p in [\"calico\", \"tricolor\", \"tortie\", \"torbie\"]):\n",
    "        is_multi = 1\n",
    "    \n",
    "    return base_primary, base_secondary, is_multi, pattern_flags\n",
    "\n",
    "# Apply color parsing to the training set\n",
    "color_info_train = train['Color'].apply(process_color)\n",
    "train['Color_Primary'] = color_info_train.apply(lambda x: x[0])\n",
    "train['Color_Secondary'] = color_info_train.apply(lambda x: x[1])\n",
    "train['IsMultiColor'] = color_info_train.apply(lambda x: x[2])\n",
    "for p in patterns:\n",
    "    train['Pattern_' + p.capitalize()] = color_info_train.apply(lambda x: x[3][p])\n",
    "\n",
    "# Apply color parsing to the test set\n",
    "color_info_test = test['Color'].apply(process_color)\n",
    "test['Color_Primary'] = color_info_test.apply(lambda x: x[0])\n",
    "test['Color_Secondary'] = color_info_test.apply(lambda x: x[1])\n",
    "test['IsMultiColor'] = color_info_test.apply(lambda x: x[2])\n",
    "for p in patterns:\n",
    "    test['Pattern_' + p.capitalize()] = color_info_test.apply(lambda x: x[3][p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ee638-ffb5-4d3b-b8b0-30d7e18cf457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location\n",
    "def extract_city(location):\n",
    "    loc = location.strip()\n",
    "    if \"Outside\" in loc:\n",
    "        return \"Outside\"\n",
    "    if \" in \" in loc:\n",
    "        city = loc[loc.rfind(\" in \") + 4 : loc.rfind(\" (\")]\n",
    "        return city\n",
    "    if loc.endswith(\"(TX)\"):\n",
    "        return loc[:loc.rfind(\" (\")]\n",
    "    return loc\n",
    "\n",
    "# Extract the found city\n",
    "train['Found_City'] = train['Found Location'].apply(extract_city)\n",
    "test['Found_City'] = test['Found Location'].apply(extract_city)\n",
    "\n",
    "# Min threashold to treat city individually\n",
    "min_threshold = 100\n",
    "city_counts = train['Found_City'].value_counts()\n",
    "cities_to_keep = city_counts[city_counts >= min_threshold].index.tolist()\n",
    "\n",
    "def group_city(city, cities_to_keep=cities_to_keep):\n",
    "    return city if city in cities_to_keep else \"Other Found City\"\n",
    "\n",
    "train['Found_City_Grouped'] = train['Found_City'].apply(group_city)\n",
    "test['Found_City_Grouped'] = test['Found_City'].apply(group_city)\n",
    "\n",
    "print(train['Found_City_Grouped'].value_counts())\n",
    "\n",
    "# Found in Austin or not\n",
    "train['Found_In_Austin'] = (train['Found_City'] == 'Austin').astype(int)\n",
    "test['Found_In_Austin'] = (test['Found_City'] == 'Austin').astype(int)\n",
    "\n",
    "train = train.drop(['Found_City'], axis=1)\n",
    "test = test.drop(['Found_City'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ed678-3ddc-4243-9181-a129c8f780f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex upon Intake\n",
    "# Split \"Sex upon Intake\" into Gender and Fixed status\n",
    "def split_sex(sex_str):\n",
    "    s = sex_str.lower()\n",
    "    if s.startswith(\"neutered\") or s.startswith(\"spayed\"):\n",
    "        fixed = \"Yes\"\n",
    "    elif s.startswith(\"intact\"):\n",
    "        fixed = \"No\"\n",
    "    else:\n",
    "        fixed = \"Unknown\"\n",
    "    if \"female\" in s:\n",
    "        gender = \"Female\"\n",
    "    elif \"male\" in s:\n",
    "        gender = \"Male\"\n",
    "    else:\n",
    "        gender = \"Unknown\"\n",
    "    return gender, fixed\n",
    "\n",
    "train[['Gender','Fixed']] = pd.DataFrame(train['Sex upon Intake'].apply(split_sex).tolist(), index=train.index)\n",
    "test[['Gender','Fixed']] = pd.DataFrame(test['Sex upon Intake'].apply(split_sex).tolist(), index=test.index)\n",
    "# Drop the original Sex column\n",
    "train = train.drop('Sex upon Intake', axis=1)\n",
    "test = test.drop('Sex upon Intake', axis=1)\n",
    "print(train[['Gender','Fixed']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c402d-3686-4a5b-82d1-53eaa1a975b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variable\n",
    "y = train['Outcome Type']\n",
    "\n",
    "# Drop all raw and unnecessary text columns\n",
    "columns_to_drop = [\n",
    "    'Outcome Type',   # only in train\n",
    "    'Breed',         \n",
    "    'Color',         \n",
    "    'Found Location', \n",
    "    'Id',             \n",
    "    'Breed_Primary',  \n",
    "    'Breed_Secondary',\n",
    "    'Color_Primary',\n",
    "    'Color_Secondary'\n",
    "]\n",
    "\n",
    "train_features = train.drop(columns=columns_to_drop, axis=1)\n",
    "test_features = test.drop(columns=[col for col in columns_to_drop if col != 'Outcome Type'], axis=1)\n",
    "\n",
    "# Now check remaining string columns\n",
    "print(\"Remaining object columns:\", train_features.select_dtypes(include=['object']).columns.tolist())\n",
    "\n",
    "# One-hot encode\n",
    "full_data = pd.concat([train_features, test_features], axis=0, ignore_index=True)\n",
    "categorical_cols = ['Animal Type','Intake Type','Intake Condition','Found_City_Grouped',\n",
    "                    'Breed_Primary_Cat','Breed_Secondary_Cat','Gender','Fixed',\n",
    "                    'IntakeYear','IntakeMonth']\n",
    "full_dummies = pd.get_dummies(full_data, columns=categorical_cols, drop_first=False)\n",
    "print(\"Total features after one-hot:\", full_dummies.shape[1])\n",
    "\n",
    "# Split encoded data back into train/test\n",
    "X_train_enc = full_dummies.iloc[:len(train_features), :].copy()\n",
    "X_test_enc = full_dummies.iloc[len(train_features):, :].copy()\n",
    "\n",
    "# Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_enc)\n",
    "X_test_scaled = scaler.transform(X_test_enc)\n",
    "\n",
    "\n",
    "y_test = test['Outcome Type']\n",
    "y_test_num = y_test.map(class_to_idx)\n",
    "test_features = test.drop(columns=[col for col in columns_to_drop if col != 'Outcome Type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aef6f9-f719-471f-96aa-9ca80f149b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb # Import for callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler # Assuming X_train_scaled is pre-scaled\n",
    "import optuna\n",
    "import functools\n",
    "\n",
    "\n",
    "classes = sorted(y.unique())\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "y_num = y.map(class_to_idx)\n",
    "print(f\"Target classes: {classes}\")\n",
    "print(f\"Encoded target mapping: {class_to_idx}\")\n",
    "\n",
    "# Compute class weights\n",
    "class_counts = y_num.value_counts().to_dict()\n",
    "num_classes = len(classes)\n",
    "total_samples = len(y_num)\n",
    "\n",
    "class_weights = {cls_idx: total_samples / (num_classes * count)\n",
    "                 for cls_idx, count in class_counts.items()}\n",
    "print(f\"Calculated Class weights: {class_weights}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Splitting data into training and validation sets for Optuna...\")\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train_scaled, y_num,\n",
    "                                            test_size=0.2,    # 20% for validation\n",
    "                                            stratify=y_num,   # Ensure class distribution is similar\n",
    "                                            random_state=42)  # For reproducibility\n",
    "print(f\"Training set shape: {X_tr.shape}, Validation set shape: {X_val.shape}\")\n",
    "\n",
    "def objective(trial, X_tr, y_tr, X_val, y_val, class_weights):\n",
    "\n",
    "    fixed_params = {\n",
    "        'objective': 'multiclass',    \n",
    "        'metric': 'multi_logloss',    \n",
    "        'n_estimators': 1000,         \n",
    "        'class_weight': class_weights,\n",
    "        'random_state': 42,           \n",
    "        'n_jobs': -1                  \n",
    "    }\n",
    "\n",
    "    \n",
    "    tuning_params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 256),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 7) # Include 0 to disable bagging\n",
    "    }\n",
    "\n",
    "\n",
    "    params = {**fixed_params, **tuning_params}\n",
    "\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    \n",
    "    early_stopping_callback = lgb.early_stopping(\n",
    "        stopping_rounds=50, \n",
    "        verbose=False        \n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Train the model with early stopping\n",
    "        model.fit(X_tr, y_tr,\n",
    "                  eval_set=[(X_val, y_val)],\n",
    "                  eval_metric='multi_logloss', # Metric monitored by early stopping\n",
    "                  callbacks=[early_stopping_callback])\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        val_preds = model.predict(X_val)\n",
    "        # Calculate balanced accuracy as the metric to maximize\n",
    "        balanced_accuracy = balanced_accuracy_score(y_val, val_preds)\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f\"Trial {trial.number} failed with error: {e}. Parameters: {trial.params}\")\n",
    "       \n",
    "        balanced_accuracy = 0.0\n",
    "   \n",
    "    return balanced_accuracy\n",
    "\n",
    "\n",
    "objective_with_data = functools.partial(objective,\n",
    "                                        X_tr=X_tr, y_tr=y_tr,\n",
    "                                        X_val=X_val, y_val=y_val,\n",
    "                                        class_weights=class_weights)\n",
    "\n",
    "print(\"Creating Optuna study...\")\n",
    "# Create an Optuna study object to maximize balanced accuracy\n",
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                            study_name=\"lgbm_multiclass_tuning\")\n",
    "\n",
    "print(f\"Starting Optuna optimization with {100} trials...\") # Adjust n_trials as needed\n",
    "# Run the optimization process\n",
    "study.optimize(objective_with_data,\n",
    "               n_trials=100, \n",
    "               timeout=600)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Best trial number: {study.best_trial.number}\")\n",
    "print(f\"Best value (Balanced Accuracy): {study.best_value:.5f}\")\n",
    "print(\"Best hyperparameters found:\")\n",
    "best_hyperparams = study.best_params\n",
    "for key, value in best_hyperparams.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "final_fixed_params = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'class_weight': class_weights,\n",
    "    'random_state': 42, #HGTTG\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 300\n",
    "}\n",
    "\n",
    "\n",
    "# best_hyperparams\n",
    "final_params = {**final_fixed_params, **best_hyperparams}\n",
    "\n",
    "\n",
    "print(\"\\nTraining final model with best hyperparameters on the full training data\")\n",
    "\n",
    "final_model = lgb.LGBMClassifier(**final_params)\n",
    "\n",
    "# Train the final model on the entire training dataset (X_train_scaled, y_num)\n",
    "final_model.fit(X_train_scaled, y_num)\n",
    "\n",
    "print(\"Final model trained successfully.\")\n",
    "\n",
    "test_preds = final_model.predict(X_test_scaled)\n",
    "print(\"\\nFinal Model Evaluation on Test Set:\")\n",
    "print(\"Balanced Accuracy on test:\", balanced_accuracy_score(y_test_num, test_preds))\n",
    "print(\"Test classification report:\")\n",
    "print(classification_report(y_test_num, test_preds, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff3887-5469-43d3-ad39-0d5d2e6c5bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
